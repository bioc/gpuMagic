---
title: "gpuMagic quick start guide"
author: 
- name: Jiefei Wang
  affiliation: Roswell Park Comprehensive Cancer Center, Buffalo, NY
date: "`r Sys.Date()`"
output:
    BiocStyle::html_document:
        toc: true
        toc_float: true
vignette: >
  %\VignetteIndexEntry{quickStart}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
package: gpuMagic
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library("gpuMagic")
#devtools::load_all() 
```
# Introduction
The gpuMagic package is designed for ultilizing the computing power of modern GPU. The package has the ability to compile the R code and implement the code on a GPU kernel. Therefore, it is easy for the user to design their own algorithm in pure R language and run the code in GPU. The package uses openCL as its parallel language so CPU parallel is also supported.

### Program setting
To getting started, it is always a good practice to check your GPU avalability.

```{r}
getDeviceList()
```
The devices listed above shows you the current avalable device in your computer, if you cannot see the device in your computer, you need to install the openCL driver obtained from the corresponding vender. The default device is the device 1, if you have multiple devices, you can query and choose your device by
```{r}
getDeviceInfo(1)
setDevice(1)
```
The package also have a GPU resources manager. You can check your memory usage at any time by calling
```{r}
gpuMagic.getMemUsage()
```
After the proper device has been selected, you are ready to write your GPU code. 

###Warning
If you are using the Nvidia graphic card, it is a good practice to change your Timeout Detection and Recovery(TDR) setting as the driver will reset the graphic card if the function takes too long to run the code. The default setting is 2 seconds, which is too short for most operations. If you are using the other device such as Intel graphic card, you are safe to continue.

The setting can be found in NVIDIA Nsight. Change it to a larger number such as 20 second will be OK for most program.

### GPU code
The GPU code should be written as an R function and should be compatible with **sapply** function. Here is an example of matrix multiplication
```{r}
#C=A%*%B
#Each time the function will compute a column of C
matMul<-function(ind,A,B){
  C=A%*%B[,ind]
  return(C)
}
```
You can test your function by using **sapply**
```{r}
n=10
m=50
k=100
A=matrix(runif(n*m),n,m)
B=matrix(runif(n*m),m,k)
C_sapply=sapply(1:k,matMul,A,B)
#The internal matrix operation function in R
C_internel=A%*%B
#Compute error
max(abs(C_sapply-C_internel))
```
As you see here, the result from sapply is consistent with the result from the R internal matrix multiplication function. To run the above function in GPU, it can be simply achieved by changing **sapply** to **gpuSapply**
```{r}
C_gpu=gpuSapply(1:k,matMul,A,B)
#Compute error
max(abs(C_gpu-C_internel))
```
That is all you need to do to run the GPU code! We can try a larger sample size to see the speed differences between GPU and R code.
```{r}
n=2000
m=2000
k=2000
A=matrix(runif(n*m),n,m)
B=matrix(runif(n*m),m,k)
system.time(
  gpuSapply(1:k,matMul,A,B)
)
system.time(
  A%*%B
)
```
In my compute the GPU(GTX 1070) code takes about 0.4 secs and CPU(AMD razen 1600) takes 4.5 secs, you get 10+ times speed up in this matrix operation! 













